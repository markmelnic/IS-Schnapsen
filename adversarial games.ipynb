{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Systems 2020: 3rd  practical assignment \n",
    "## Games and Adversarial Search \n",
    "### An introduction to the Schnapsen platform\n",
    "\n",
    "Your name: Mark Melnic\n",
    "\n",
    "Your VUnetID: mmc560\n",
    "\n",
    "If you do not provide your name and VUnetID we will not accept your submission. \n",
    "\n",
    "### Learning objectives\n",
    "At the end of this exercise you should be able to use the Schnapsen platform, run basic games between agents, and run tournaments in order to evaluate rational agents (also called bots). You should also be able to identify the working patterns of the MiniMiax algorithm in this platform and the improvements with Alpha/Beta pruning. \n",
    "\n",
    "1. Understand the main functionality of the Schnapsen platform (playing games and running tournements)\n",
    "2. Implement your own rational agents (bots)\n",
    "2. Follow the individual steps and explain the MiniMax algorithm\n",
    "3. Make small modifications of the code to see the effect on the search algorithms\n",
    "4. Make small adaptations to the algorithm to study the computational properties\n",
    "\n",
    "### Practicalities\n",
    "\n",
    "Follow this Notebook step-by-step. \n",
    "\n",
    "Of course, you can do the exercises in any Programming Editor of your liking. But you do not have to. Feel free to simply write code in the Notebook. Please use your studentID+Assignment3.ipynb as the name of the Notebook.  \n",
    "\n",
    "Note: unlike the courses dedicated to programming we will not evaluate the style of the programs. But we will, however, test your programs on other data that we provide, and your program should give the correct output to the test-data as well.\n",
    "\n",
    "As was mentioned, the assignment is graded as pass/fail. To pass you need to have either a full working code or an explanation of what you tried and what didn't work for the tasks that you were unable to complete (you can use multi-line comments or a text cell).\n",
    "\n",
    "## Initialising \n",
    "\n",
    "First, let us make sure the necessary packages are installed, and imported. Run the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random\n",
    "\n",
    "from api import State, engine, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing the first games\n",
    "\n",
    "The basic engine comes with three basic bots: rand, bully and rdeep (the rest you can ignore for now). To try them out, just run the following bit of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your first player\n",
    "player1 = \"rand\"\n",
    "player2 = \"bully\"\n",
    "# Decide in which phase you want to start the game. \n",
    "startphase = 1\n",
    "# Decide whether you want verbose output or not \n",
    "verbose=True \n",
    "\n",
    "#And here you run a game on the engine. \n",
    "engine.play(util.load_player(player1),util.load_player(player2), state=State.generate(phase=startphase), max_time=10000, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running engine.play provides some textual output to show how the game is progressing. At every plie (half a turn) you will see what move the player made and a concise overview of the board. Something like this (when you run a game in verbose mode):\n",
    "\n",
    "> Player 1 plays: KC\n",
    "> The game is in phase: 1<br>\n",
    "> Player 1's points: 21, pending: 0<br>\n",
    "> Player 2's points: 25, pending: 0<br>\n",
    "> Player 1's hand: 10C JC AD QD QH<br>\n",
    "> Player 2's hand: AC KD 10H KS QS<br>\n",
    "There are 2 cards in the stock<br>\n",
    "\n",
    "\n",
    "The first line signifies that player 1 has played the King of Clubs card. Internally these cards are represented by indices from 0 to 19. To make the translation between indices and card names, use util.get_card_name(index), which returns the rank and the suit of the card as a tuple. Alternatively, use util.get_rank(index) and util.get_suit(index) for each property alone. In this case it is a King of Clubs. Have a look at the GitHub readme or at the top of __deck.py to see the convention used for encoding cards into indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.get_card_name(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run the Python programmes provided from the command line, or in your favourite editor. <br>\n",
    "Run \n",
    "> python play.py -1 rand -2 bully \n",
    "\n",
    "to run the rand bot against the bully bot, or \n",
    "> python play.py -h \n",
    "\n",
    "to see other options. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of randomness involved in the game when the cards are distributed to the players and the pile. To get an accurate sense of whether one player is better than another, you'll need to play a number of different games. The following code will play a tournament between bully and rand where every pair of participants plays 10 matches. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "botnames = []\n",
    "verbose = False \n",
    "myphase = 1\n",
    "myrepeats = 10\n",
    "\n",
    "# Create player 1\n",
    "player1 = util.load_player(\"rand\") \n",
    "player2 = util.load_player(\"bully\") \n",
    "\n",
    "bots = [player1,player2]\n",
    "\n",
    "n = len(bots)\n",
    "wins = [0] * len(bots)\n",
    "matches = [(p1, p2) for p1 in range(n) for p2 in range(n) if p1 < p2]\n",
    "\n",
    "totalgames = (n*n - n)/2 * myrepeats\n",
    "playedgames = 0\n",
    "\n",
    "print('Playing {} games:'.format(int(totalgames)))\n",
    "for a, b in matches:\n",
    "    for r in range(myrepeats):\n",
    "\n",
    "        if random.choice([True, False]):\n",
    "            p = [a, b]\n",
    "        else:\n",
    "            p = [b, a]\n",
    "\n",
    "        # Generate a state with a random seed\n",
    "        state = State.generate(phase=myphase)\n",
    "\n",
    "        winner, score = engine.play(bots[p[0]], bots[p[1]], state, 1000, verbose, True)\n",
    "\n",
    "        if winner is not None:\n",
    "            winner = p[winner - 1]\n",
    "            wins[winner] += score\n",
    "\n",
    "        playedgames += 1\n",
    "        print('Played {} out of {:.0f} games ({:.0f}%): {} \\r'.format(playedgames, totalgames, playedgames/float(totalgames) * 100, wins))\n",
    "\n",
    "print('Results:')\n",
    "for i in range(len(bots)):\n",
    "    print('    bot {}: {} points'.format(bots[i], wins[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: \n",
    "The previous code runs a tournament between rand and bully, but you can adapt the script by testing the performance of these bots with the third default bot, rdeep, as the opponent. The general idea of rdeep was extensively discussed under the header PIMS (Perfect Information Monte Carlo Sampling). Report in the following Markdown Cell on the results you get from two-player tournaments including rdeep, rand and bully (rdeep vs. rand; rdeep vs. bully). Describe which games you played, and who won. Add the (non-verbose) output of the script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: \n",
    "The previous code runs a tournament between two bots only, but you can easily adapt the script above to play round-robin tournament. All you have to do is to add a third player to the bots list. Report in the following Markdown Cell on the results you get from three-player tournament including rdeep, rand and bully. Add the (non-verbose) output of the script. Report on the results of the tournament and try to explain in your own words what do the results mean.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's have a look at how the bots work. Open the file bots/rand/rand.py in PyCharm. Each bot is a class called Bot. \n",
    "\n",
    "> We will use more advanced features of Python than what you have seen so far in Introduction to Python (donâ€™t worry), so for more details have a look at:\n",
    ">    https://www.learnpython.org/en/Classes_and_Objects\n",
    "\n",
    "The rand bot contains nothing but an empty constructor, and one method: get_move(self, state). This is the only method you need to implement to get a working bot. It receives a description of a game state, and returns a move. A move is always a pair of two elements, each of which can be either an integer or None. Note that it is not an option to pass, therefore (None, None) is not a valid move.\n",
    "\n",
    "As you can see, in the rand bot, the state object does almost all the work: state.moves() gives you a list of legal moves. The rand Bot simply makes a random choice from this list using the function random.choice() from the python library.\n",
    "\n",
    "If you want to see what happens when you make a given move, just do\n",
    "next_state = state.next(move)\n",
    "And you get a state representing the outcome.\n",
    "\n",
    "What else can the State object do for you? You can look at the code in api/_state.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bully.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bully is a deterministic bot: given the same state it will always do the same thing. We've removed part of the explanation from the comments. \n",
    "\n",
    "### Task 3: \n",
    "Have a look at the code: describe in your own words what strategy does the bully bot use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rdeep.py\n",
    "The lectures discuss the hill-climbing strategy: look one move ahead and pick the move that leads to the best heuristic. The heuristics we use is the ratio of the player points w.r.t. to the total points currently assigned in the game. The higher this value, the better the state is for us. Imagine doing hill-climbing with this heuristic. This strategy would not work here. Why not? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid this issue, we need to loook further ahead than the hill climbing strategy does. rdeep.py does this in the simplest way we could think of. Make eight random moves and look at the value of the resulting state. Do this a few times and average the values found. This method is called Perfect-Information Monte-Carlo Sampling (PIMC).\n",
    "\n",
    "You just ran a tournament between rdeep and the other two bots. Most likely, rdeep will have won a few more games. But does the difference really mean rdeep is better? It might just be that rdeep is no better than rand and won by pure luck.\n",
    "\n",
    "### Task 4 \n",
    "If you wanted to provide scientific evidence that rdeep is better than rand, how would you go about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mybot.py\n",
    "\n",
    "It's time to write your own bot. Think of a simple strategy that is easy to implement. To create the bot follow these steps:\n",
    "1. Create the directory bots/mybot (in the directory, not this notebook!)\n",
    "2. Add an empty file __init__.py, or copy it from one of the other bot directories.\n",
    "3. Copy rand.py to the directory mybot, and rename it to mybot.py\n",
    "4. Change the implementation of get_move(state). Keep the method signature (line 16) exactly as it is.\n",
    "\n",
    "Make sure your bot always returns either a pair elements that are each either int or None. Try playing a tournament against rand. See if you can get a decent margin.\n",
    "\n",
    "If your bot has parameters (like a search depth, or a pre-programmed probability of doing nothing) you can add these to the constructor. Have a look at rdeep.py to see how this is done.\n",
    "\n",
    "To get some examples of how to talk to the API, see the README.md\n",
    "\n",
    "### Task 5 \n",
    "Add your implementation of get_move() and the result of a tournament against rand to your report. \n",
    "\n",
    "Please write your code here (in raw text, to avoid an error), as well as the results in the following cell: \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MINIMAX Adversarial Games\n",
    "Now we will build some bots that search the game tree by using the MiniMax algorithm and show genuinely intelligent behavior. Because we only have partial information, and as there would be too many belief-states, we play these bots on the 2nd phase of the game only, when the stock has been exhausted, such that the state of all cards is known and no assumptions have to be made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All you need to do to finish the minimax bot is to add one line of code on line 58. The skeleton given in the code differs slightly from the pseudocode from the lecture in that instead of having two sub-methods, all is implemented in one method. Take your time to really understand the minimax algorithm, recursion, and the rest of the code. The pseudocode would then look like as follows "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "function VALUE(state) returns a utility value, move \n",
    "  # First we initialise the values for best_value (which we need to calculate the MIN and MAX later.   \n",
    "  best_value = -infinity if MAXMOVE, +infinity if MINMOVE\n",
    "  # then we check for the terminal states of our search tree, either if there are no more successors, or in case we ran over the maximal depths (set in minimax.py)\n",
    "  if TERMINAL-TEST(state) then return UTILITY(state)\n",
    "  if DEPTH = max_depth then return heuristic_value(state)\n",
    "\n",
    "  # Now we calculate the values of the successor states, and choose the maximal one (or minimal) depending in which state we are. \n",
    "  for move in SUCCESSORS(state) do \n",
    "    next_move = SUCCESSOR(move)\n",
    "    value = VALUE(next_move)   # This is a recursive call, in which we calculate the optimal value for the opponent to play\n",
    "\n",
    "    if MAXMOVE # This is information provided by the game-engine as a function in the state class. \n",
    "        best_value is now calculated as the maximum between the value of the next_move and the previously calculated value (!)\n",
    "        best_move is set to the the move with the current best value  (!)\n",
    "    else \n",
    "        best_value is now calculated as the minimum between the value of the next_move and the previously calculated value  (!)\n",
    "        best_move is set to the the move with the current best value  (!) \n",
    "  return best_value, best_move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6\n",
    "Implement the parts of the algorithm marked \"???\" in the code of the minimax.py program in the bot directory. Please write down your new code in the following raw cell. \n",
    "\n",
    "Once you have done this, you can check whether you algorithm runs correctly."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your first player\n",
    "player1 = \"rand\"\n",
    "player2 = \"minimax\"\n",
    "# Decide in which phase you want to start the game. \n",
    "startphase = 2\n",
    "# Decide whether you want verbose output or not \n",
    "verbose=True \n",
    "\n",
    "#And here you run a game on the engine. \n",
    "engine.play(util.load_player(player1),util.load_player(player2), state=state.generate(phase=startphase), max_time=10000, verbose=verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heuristics \n",
    "\n",
    "What heuristic do these implementations use? Explain the heuristic function in your own words: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapt the search depth (in the constructor of the Minimax bot, and adapt the Minimax procedure by uncommenting the respective function) to smaller values (search depths). Report on the performance of these different versions of using the heuristics against some other players (like rdeep and rand), and see if the performance (nr. of games won) differs with the different look-aheads. \n",
    "\n",
    "### Task 7\n",
    "Report on your experiments with Minimax as compared to previous bots (or a copy of MiniMax with a different search depths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alphabeta\n",
    "\n",
    "Finally let us look at Alphabeta pruning. Alphabeta pruning is a technique to make minimax faster. If implemented correctly, it will do exactly the same  thing minimax does, but skip large parts of the search tree. We've provided a basic implementation in bots/alphabeta/alphabeta.py. \n",
    "\n",
    "### Task 8\n",
    "\n",
    "Once again, one crucial bit of the implementation are missing, the decision on when to prune. Finish the implementation of the alphabeta bot. Copy the line of code you adapted in the skeleton file alphabeta.py into the following cell: \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following programme lets you see if you implemented alphabeta and minimax correctly. Run it, and check the outcome. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api import State, util\n",
    "import random, time\n",
    "\n",
    "from bots.alphabeta import alphabeta\n",
    "from bots.minimax import minimax\n",
    "\n",
    "REPEATS = 100\n",
    "DEPTH = 6\n",
    "\n",
    "ab = alphabeta.Bot(randomize=False, depth=DEPTH)\n",
    "mm = minimax.Bot(randomize=False, depth=DEPTH)\n",
    "\n",
    "mm_time = 0\n",
    "ab_time = 0\n",
    "\n",
    "# Repeat\n",
    "for r in range(REPEATS):\n",
    "    \n",
    "    # Repeat some more \n",
    "    for r2 in range(REPEATS):\n",
    "\n",
    "        # Generate a starting state\n",
    "        state = State.generate(phase=2)\n",
    "\n",
    "        # Ask both bots their move\n",
    "        # (and time their responses)\n",
    "\n",
    "        start = time.time()\n",
    "        mm_move = mm.get_move(state)\n",
    "        mm_time += (time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "        ab_move = ab.get_move(state)\n",
    "        ab_time += (time.time() - start)\n",
    "\n",
    "\n",
    "        if mm_move != ab_move:\n",
    "            print('Difference of opinion! Minimax said: {}, alphabeta said: {}. State: {}'.format(mm_move, ab_move, state))\n",
    "        else:\n",
    "            print('Agreed.')\n",
    "\n",
    "print('Done. time Minimax: {}, time Alphabeta: {}.'.format(mm_time/REPEATS, ab_time/REPEATS))\n",
    "print('Alphabeta speedup: {} '.format(mm_time/ab_time))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit",
   "language": "python",
   "name": "python38164bit4531da3462464465a8c5836a34dda191"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}